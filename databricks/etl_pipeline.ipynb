{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "24eb0606-18ba-4917-8479-eec84424cdbd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# ETL Pipeline for \n",
    "# ============================================================\n",
    "\n",
    "# 1. EXTRACT\n",
    "# Read the uploaded CSV file from /FileStore\n",
    "df = spark.table(\n",
    "    \"workspace.default.final_df\"\n",
    ")\n",
    "\n",
    "# 2. TRANSFORM\n",
    "from pyspark.sql.functions import col, when, round as spark_round, mean as spark_mean\n",
    "\n",
    "# Remove nulls from key columns\n",
    "df = df.dropna(subset=[\"Weekly_Sales\", \"Store\"])\n",
    "\n",
    "# Standardize column names\n",
    "for old_name in df.columns:\n",
    "    new_name = old_name.replace(\"_\", \"\").replace(\" \", \"\")\n",
    "    if old_name != new_name:\n",
    "        df = df.withColumnRenamed(old_name, new_name)\n",
    "\n",
    "# Fill nulls in MarkDown columns with 0\n",
    "markdown_cols = [\"MarkDown1\", \"MarkDown2\", \"MarkDown3\", \"MarkDown4\", \"MarkDown5\"]\n",
    "for col_name in markdown_cols:\n",
    "    if col_name in df.columns:\n",
    "        df = df.withColumn(col_name, when(col(col_name).isNull(), 0).otherwise(col(col_name)))\n",
    "\n",
    "# Feature Engineering\n",
    "df = df.withColumn(\"DailySales\", spark_round(col(\"WeeklySales\") / 7, 2))\n",
    "\n",
    "# Calculate Discount as mean of MarkDown columns\n",
    "df = df.withColumn(\n",
    "    \"Discount\",\n",
    "    spark_round(\n",
    "        sum([col(c) for c in markdown_cols]) / len(markdown_cols),\n",
    "        2\n",
    "    )\n",
    ")\n",
    "\n",
    "display(df.head(10))\n",
    "\n",
    "# 3. LOAD\n",
    "# Register as a Delta table for SQL querying\n",
    "df.write.mode(\"overwrite\").saveAsTable(\"workspace.default.df_ready\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1d585f55-6391-405e-bcc8-8a5d11207d13",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "etl_pipeline",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}